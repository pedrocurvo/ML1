\documentclass[12pt,a4paper,oneside]{paper}
\input{./preamble}

\addbibresource{bibliography.bib}
\begin{document}
\pagestyle{plain}


% Title
\def\title {Homework Assignment 2}

% Input the cover
\input{cover} 
\cleardoublepage
\fontfamily{cmr}\selectfont
\setcounter{page}{0}
\tableofcontents
\newpage

\small

\section{Naive Bayes Modeling of Climate}

\subsection*{a)}

Our dataset consists of $N$ samples, each of which composed by $\bm{x} \in \mathbb{R}^{D}$ features
and a target variable $t \in \mathbb{R}^K$, where $K$ is the set of possible classes. $t$ is a one-hot encoded vector,
where the $k$-th element is 1 if the sample belongs to class $k$ and 0 otherwise.
Thus, we can write the likelihood of the data, without assuming Naive Bayes, as:

\begin{align*}
    p(\bm{T}, \bm{X} | \bm{M}, \bm{B}) &= \prod_{n=1}^{N} p(\bm{t}_n, \bm{x}_n | \bm{M}, \bm{B}) \quad \text{assuming i.i.d samples} \\
    &= \prod_{n=1}^{N} p(\bm{x_n} | \bm{t_n}, \bm{M}, \bm{B}) p(\bm{t_n} | \bm{M}, \bm{B}) \quad \text{, making use of the product rule} \\
    &= \prod_{n=1}^{N} \prod_{k=1}^{K} [p(\bm{x_n} | t_{nk} = 1, \bm{M}, \bm{B}) p(t_{nk} = 1 | \bm{M}, \bm{B})]^{t_{nk}} \\
    &= \prod_{n=1}^{N} \prod_{k=1}^{K} [p(\bm{x_n} | C_k, \bm{M}, \bm{B}) p(C_k | \bm{M}, \bm{B})]^{t_{nk}} \quad \text{, since} \quad p(C_k | \bm{M}, \bm{B}) = p(t_{nk} = 1 | \bm{M}, \bm{B}) \\
    &= \prod_{n=1}^{N} \prod_{k=1}^{K} [\pi_k p(\bm{x_n} | C_k, \bm{M}, \bm{B})]^{t_{nk}} \quad \text{, since} \quad p(C_k | \bm{M}, \bm{B}) = \pi_k
\end{align*}

Now, assuming the Naive Bayes assumption, we have that the features are conditionally independent given the class, i.e. $p(\bm{x_n} | C_k, \bm{M}, \bm{B}) = \prod_{d=1}^{D} p(x_{nd} | C_k, \bm{M}, \bm{B})$, 
which allows us to write the likelihood as:

\begin{align*}
    p(\bm{T}, \bm{X} | \bm{M}, \bm{B}) &= \prod_{n=1}^{N} \prod_{k=1}^{K} [\pi_k \prod_{d=1}^{D} p(x_{nd} | C_k, \bm{M}, \bm{B})]^{t_{nk}}
\end{align*}

\newpage
\subsection*{b)}

Assuming that the features are modelled as Gaussian Distributions, i.e., a Multivariate Gaussian in the case of dependent features and Univariate Gaussians in the case of independent features, then we can assume the following for each
one of the assumptions: 

\subsubsection*{Bayes Classifier:}

Since the features in $\bm{x}$ are not independent, then their relation could be explained by a multivariate Gaussian distribution.
This multivariate gaussian distribution, for each class, would have a mean $\bm{\mu}_k$ and a covariance matrix $\bm{\Sigma}_k$, with
$\bm{\mu}_k \in \mathbb{R}^{D}$ and $\bm{\Sigma}_k \in \mathbb{R}^{D \times D}$, however since $\bm{\Sigma}_k$ is a covariance matrix, it is symmetric.
Thus, the total number of parameters per class would be $D + \frac{D(D+1)}{2}$, where the first term corresponds to the parameter of the mean and the second term to the ones of the covariance matrix, 
which are the elements of the upper triangular part of the matrix and the diagonal.

Thus, for the entire number of classes, the total number of parameters would be $K \times (D + \frac{D(D+1)}{2})$.
Adding to this, we would need to consider the prior probabilities for each class, which would add $K - 1$ parameters, since the last class can be inferred from the others.
Thus, the total number of parameters would be:

$$
K \times (D + \frac{D(D+1)}{2}) + K - 1 \quad \text{parameters, assuming a Bayes Classifier}
$$

\subsubsection*{Naive Bayes Classifier:}

Now, if we assume that the features are independent, and thus $p(\bm{x_n} | C_k, \bm{M}, \bm{B}) = \prod_{d=1}^{D} p(x_{nd} | C_k, \bm{M}, \bm{B})$,
with $p(x_{nd} | C_k, \bm{M}, \bm{B}) = \mathcal{N}(x_{nd} | \mu_{kd}, \sigma_{kd}^2)$, since the features are modelled as Gaussian distributions. 

Then, the total number of parameters per class would be $2D$: 
\begin{itemize}
    \item $D$ parameters that correspond to the mean for each feature.
    \item $D$ parameters that correspond to the variance for each feature.
\end{itemize}

We can also think this as the following: on the previous example of the Bayes Classifier, we had a covariance matrix that had $D + \frac{D(D+1)}{2}$ parameters, since the 
upper triangular part of the matrix was filled. Now, since the features are independent, the covariance matrix would be a diagonal matrix, which would have only $D$ parameters, because
the diagonal of $\bm{\Sigma}_k$ would be the variance of each feature and the other elements would be zero, since they correspond to the covariance between the features, which in this case: $Cov(x_{ni}, x_{nj}) = 0$ for $i \neq j$.

Thus, for the entire number of classes, the total number of parameters would be:
 $$
 K \times 2D
 $$

For the prior probabilities, we would need $K - 1$ parameters, as in the previous case, since the last class can be inferred from the others.

This brings the total number of parameters to:

$$
K \times 2D + K - 1 \quad \text{parameters, assuming a Naive Bayes Classifier}
$$

As we can see, Naive Bayes reduces the complexity of the model from a quadratic to a linear number of parameters, which makes it a more efficient model, but with the cost of the assumption that the features are independent.

\subsubsection*{Naive}

The term “naive” in the Naive Bayes classifier refers to the assumption that features are conditionally
independent given the class label, meaning that the features do not influence each other.
This assumption usually falls in real-world, where correlated features are common.
For instance, in the climate set described above, regions could be correlated based on their location.
If one region is in the southern hemisphere and another in the northern hemisphere,
their seasons would be inverted, which means they have a negative correlation.

Another example, starting from the weather above, could be that given two features: $X = \text{temperature}$ and
$Y = \text{snow}$. The Naive Bayes classifier would calculate the probability of both 40°C and snow occurring
by multiplying their individual probabilities. In reality, this is wrong because the joint probability would be 0,
as snow does not happen at that temperature and the probability is actually 0.

Another example that can be used to illustrate this is, for example, the detection of spam. i.e., imagine that 
you have features that are the presence of certain words in the email, for example, "win", "money", etc. You know
that when they appear together, the email is more likely to be spam, thus the features are correlated.

% Note: In the case of the temperature above, the features might be correlated, as I said in the example before, 
% so basically we are using a multivariate Gaussian distribution with a diagonal covariance matrix, meaning that 
% in the hyperspace of the features, the distribution aligns with the axis and cannot have a diagonal elipse shape.
% Considering the Bayes classifier, we would have a full covariance matrix, which would allow the distribution to have a diagonal elipse shape. 
% In the case of the Gaussian, we could approximate the full covariance case with a multivariate combination of diagonal covariance matrices. 
% Imagine you have a 2D space, and you want to approximate a diagonal elipse, you could use two 1D elipses, one for each axis, and combine them to form the diagonal elipse.
% This could be a average case in the number of parameters since you would multipy the number of parameters of the bayes classifier by the number of gaussians you would use. 
% Which, for a certain number of gaussians, could be less than the number of parameters of the bayes classifier. 

\newpage
\subsection*{c)}

Assuming the Naive Bayes assumption, we can write the likelihood as we showed in the first question:

\begin{align*}
    p(\bm{T}, \bm{X} | \bm{M}, \bm{B}) &= \prod_{n=1}^{N} \prod_{k=1}^{K} [\pi_k \prod_{d=1}^{D} p(x_{nd} | C_k, \bm{M}, \bm{B})]^{t_{nk}}
\end{align*}

Assuming the features are modelled as Gaussian distributions, we can write the likelihood as:

\begin{align*}
    p(\bm{T}, \bm{X} | \bm{M}, \bm{B}) &= \prod_{n=1}^{N} \prod_{k=1}^{K} [\pi_k \prod_{d=1}^{D} \frac{\beta_{dk}^{\frac{1}{2}}}{\sqrt{2\pi}} \exp\left(-\frac{\beta_{dk}}{2} (x_{nd} - \mu_{dk})^2\right)]^{t_{nk}}
\end{align*}

Taking the logarithm of the likelihood, we have:

\begin{align*}
    \log p(\bm{T}, \bm{X} | \bm{M}, \bm{B}) &= \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \left[\log \pi_k + \sum_{d=1}^{D} \left[\frac{1}{2} \log \beta_{dk} - \frac{1}{2} \log 2\pi - \frac{\beta_{dk}}{2} (x_{nd} - \mu_{dk})^2\right]\right]
\end{align*}

\newpage
\subsection*{d)}

Solving for the MLE estimator for $\mu_{dk}$, we have:

\begin{align*}
    \frac{\partial \log p(\bm{T}, \bm{X} | \bm{M}, \bm{B})}{\partial \mu_{ij}} &= 0 \\
    \frac{\partial}{\partial \mu_{ij}} \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} &\left[\log \pi_k + \sum_{d=1}^{D} \left[\frac{1}{2} \log \beta_{dk} - \frac{1}{2} \log 2\pi - \frac{\beta_{dk}}{2} (x_{nd} - \mu_{dk})^2\right]\right] = 0 \\
\end{align*}

Since the only terms that depend on $\mu_{ij}$ are the ones when $k=j$ and $d=i$, we have:
\begin{align*}
    \frac{\partial}{\partial \mu_{ij}} \sum_{n=1}^{N} t_{nj} \left[\log \pi_j \frac{1}{2} \log \beta_{ij} - \frac{1}{2} \log 2\pi - \frac{\beta_{ij}}{2} (x_{nj} - \mu_{ij})^2\right] &= 0 \\
    \sum_{n=1}^{N} t_{nj} \left[ \beta_{ij} (x_{nj} - \mu_{ij})\right] &= 0 \\
    \sum_{n=1}^{N} t_{nj} \beta_{ij} x_{ni} - \sum_{n=1}^{N} t_{nj} \beta_{ij} \mu_{ij} &= 0 \\
    \sum_{n=1}^{N} t_{nj} \beta_{ij} x_{ni} &= \sum_{n=1}^{N} t_{nj} \beta_{ij} \mu_{ij} \\
    \sum_{n=1}^{N} t_{nj} x_{ni} &= \sum_{n=1}^{N} t_{nj} \mu_{ij} \\
    \mu_{ij} &= \frac{\sum_{n=1}^{N} t_{nj} x_{ni}}{\sum_{n=1}^{N} t_{nj}}
\end{align*}

Hence, 
$$
\mu_{dk_{MLE}} = \frac{\sum_{n=1}^{N} t_{nk} x_{nd}}{\sum_{n=1}^{N} t_{nk}}
$$

Thus, assuming that $t_{nk}$ is a binary variable that takes the value $1$ when $x_n \in C_k$, we can see that the MLE estimator for $\mu_{dk}$ is the average of samples that belong to class $k$, i.e, 
it represents a weighted average of the samples that belong to class $k$.

\newpage
\subsection*{e)}

Specifying $p(C_1 | \bm{x})$ for the general $k$ classes of the Naive Bayes classifier, we have:

\begin{align*}
    p(C_1 | \bm{x}) &= \frac{p(\bm{x} | C_1) p(C_1)}{p(\bm{x})} \quad \text{, by Bayes' Theorem} \\
    &= \frac{p(\bm{x} | C_1) p(C_1)}{\sum_{k=1}^{K} p(\bm{x} | C_k) p(C_k)} \quad \text{, by the law of total probability} \\
    &= \frac{\pi_1 \prod_{d=1}^{D} p(x_d | C_1)}{\sum_{k=1}^{K} \pi_k \prod_{d=1}^{D} p(x_d | C_k)} \quad \text{, by the Naive Bayes assumption that the features are i.i.d.} \\
    &= \frac{\pi_1 \prod_{d=1}^{D} p(x_d | C_1, \mu_{d1}, \beta_{d1})}{\sum_{k=1}^{K} \pi_k \prod_{d=1}^{D} p(x_d | C_k, \mu_{dk}, \beta_{dk})} \quad \text{, by the Gaussian assumption} \\
\end{align*}

\newpage
\subsection*{f)}

Firstly, we need to bare in mind that, contrary to a logistic regression, which is a discriminative model and
models the posterior probability $p(C_k | \bm{x})$ without the need to explicitly model the likelihood $p(\bm{x} | C_k)$, caring only about the decision boundary, the Naive Bayes
classifier models the joint probability $p(C_k, \bm{x})$. Summing this idea: the Naive Bayes classifier learns the joint probability
which allows to then sample from it, while the logistic regression does not care about the distribution of the data. 

Looking at our previous answer, we can see that the Naive Bayes Classifier models the joint probability
$p(\bm{x}, C_1) = p(\bm{x} | C_1) p(C_1)$ by first generating a class target $C_1$ based on the information of the prior and
then generating the features $\bm{x}$ based on the information of the likelihood $p(\bm{x} | C_1)$. 

\newpage
\subsection*{g)}

Writing $p(C_1 | \bm{x})$ for the normally distributed model explicitly, and considering the assumptions of the previous questions, we have:

\begin{align*}
    p(C_1 | \bm{x}) &= \frac{p(\bm{x} | C_1) p(C_1)}{p(\bm{x})} \quad \text{, by Bayes' Theorem} \\
    &= \frac{p(\bm{x} | C_1) p(C_1)}{\sum_{k=1}^{K} p(\bm{x} | C_k) p(C_k)} \quad \text{, by the law of total probability} \\
    &= \frac{\pi_1 \prod_{d=1}^{D} p(x_d | C_1)}{\sum_{k=1}^{K} \pi_k \prod_{d=1}^{D} p(x_d | C_k)} \quad \text{, by the Naive Bayes assumption that the features are i.i.d.} \\
    &= \frac{\pi_1 \prod_{d=1}^{D} p(x_d | C_1, \mu_{d1}, \beta_{d1})}{\sum_{k=1}^{K} \pi_k \prod_{d=1}^{D} p(x_d | C_k, \mu_{dk}, \beta_{dk})} \quad \text{, by the Gaussian assumption} \\
    &= \frac{\pi_1 \prod_{d=1}^{D} \frac{\beta_{d1}^{\frac{1}{2}}}{\sqrt{2\pi}} \exp\left(-\frac{\beta_{d1}}{2} (x_d - \mu_{d1})^2\right)}{\sum_{k=1}^{K} \pi_k \prod_{d=1}^{D} \frac{\beta_{dk}^{\frac{1}{2}}}{\sqrt{2\pi}} \exp\left(-\frac{\beta_{dk}}{2} (x_d - \mu_{dk})^2\right)} \\
\end{align*}

\newpage
\subsection*{h)}

A datapoint $\bm{x}$ is classified as $C_1$ if: 
$$
p(C_1 | \bm{x}) > p(C_k | \bm{x}) \quad \text{,}\forall k \neq 1
$$

\newpage
\subsection*{i)}

Considering the inequality $p(C_1 | \bm{x}) > p(C_k | \bm{x})$, $\forall k \neq 1$, we can write it as:

\begin{align*}
    p(C_1 | \bm{x}) &> p(C_k | \bm{x}) \\
    \frac{\pi_1 \prod_{d=1}^{D} p(x_d | C_1, \mu_{d1}, \beta_{d1})}{\sum_{k=1}^{K} \pi_k \prod_{d=1}^{D} p(x_d | C_k, \mu_{dk}, \beta_{dk})} &> \frac{\pi_k \prod_{d=1}^{D} p(x_d | C_k, \mu_{dk}, \beta_{dk})}{\sum_{k=1}^{K} \pi_k \prod_{d=1}^{D} p(x_d | C_k, \mu_{dk}, \beta_{dk})} \\
\end{align*}

Since the denominator is the same for both sides and it is always positive since it is a sum of probabilities and the probabilities are positive we can write it as:

\begin{align*}
    \pi_1 \prod_{d=1}^{D} p(x_d | C_1, \mu_{d1}, \beta_{d1}) &> \pi_k \prod_{d=1}^{D} p(x_d | C_k, \mu_{dk}, \beta_{dk})
\end{align*}

Which, assuming the Gaussian Distributions, we can write as:

\begin{align*}
    \pi_1 \prod_{d=1}^{D} \frac{\beta_{d1}^{\frac{1}{2}}}{\sqrt{2\pi}} \exp\left(-\frac{\beta_{d1}}{2} (x_d - \mu_{d1})^2\right) &> \pi_k \prod_{d=1}^{D} \frac{\beta_{dk}^{\frac{1}{2}}}{\sqrt{2\pi}} \exp\left(-\frac{\beta_{dk}}{2} (x_d - \mu_{dk})^2\right)
\end{align*}

To have quadratic inequality in the form $ax^2 + bx + c > 0$, we can take the logarithm of both sides, since if $f(x) > g(x)$, then $\log f(x) > \log g(x) \forall f(x),g(x) > 0$:

\begin{center}
\begin{align*}
    \log \pi_1 + \sum_{d=1}^{D} \left[\frac{1}{2} \log \beta_{d1} - \frac{1}{2} \log 2\pi - \frac{\beta_{d1}}{2} (x_d - \mu_{d1})^2\right] > \log \pi_k &+ \sum_{d=1}^{D} \left[\frac{1}{2} \log \beta_{dk} - \frac{1}{2} \log 2\pi - \frac{\beta_{dk}}{2} (x_d - \mu_{dk})^2\right] \\
    \log \pi_1 + \sum_{d=1}^{D} \left[\frac{1}{2} \log \beta_{d1} - \frac{1}{2} \log 2\pi - \frac{\beta_{d1}}{2} (x_d - \mu_{d1})^2\right] - \log \pi_k - \sum_{d=1}^{D} &\left[\frac{1}{2} \log \beta_{dk} - \frac{1}{2} \log 2\pi - \frac{\beta_{dk}}{2} (x_d - \mu_{dk})^2\right] > 0 \\
    \log \frac{\pi_1}{\pi_k} + \sum_{d=1}^{D} \left[\frac{1}{2} \log \frac{\beta_{d1}}{\beta_{dk}} - \frac{\beta_{d1}}{2} (x_d - \mu_{d1})^2 + \frac{\beta_{dk}}{2} (x_d - \mu_{dk})^2\right] &> 0 \\
    2 \log \frac{\pi_1}{\pi_k} + \sum_{d=1}^{D} \left[\log \frac{\beta_{d1}}{\beta_{dk}} - \beta_{d1} (x_d - \mu_{d1})^2 + \beta_{dk} (x_d - \mu_{dk})^2\right] &> 0 \\
    \sum_{d=1}^{D} \left[ - \beta_{d1} (x_d - \mu_{d1})^2 + \beta_{dk} (x_d - \mu_{dk})^2\right] &> -2 \log \frac{\pi_1}{\pi_k} - \sum_{d=1}^{D} \left[\log \frac{\beta_{d1}}{\beta_{dk}}\right] \\
    \sum_{d=1}^{D} \left[ - \beta_{d1} x_d^2 + 2 \beta_{d1} x_d \mu_{d1} - \beta_{d1} \mu_{d1}^2 + \beta_{dk} x_d^2 - 2 \beta_{dk} x_d \mu_{dk} + \beta_{dk} \mu_{dk}^2\right] &> -2 \log \frac{\pi_1}{\pi_k} - \sum_{d=1}^{D} \left[\log \frac{\beta_{d1}}{\beta_{dk}}\right] \\
    \sum_{d=1}^{D} \left[ x_d^2 (\beta_{dk} - \beta_{d1}) + 2 x_d (\beta_{d1} \mu_{d1} - \beta_{dk} \mu_{dk}) + \beta_{d1} \mu_{d1}^2 - \beta_{dk} \mu_{dk}^2\right] &> -2 \log \frac{\pi_1}{\pi_k} - \sum_{d=1}^{D} \left[\log \frac{\beta_{d1}}{\beta_{dk}}\right] \\
    \sum_{d=1}^{D} \left[ x_d^2 (\beta_{dk} - \beta_{d1}) + 2 x_d (\beta_{d1} \mu_{d1} - \beta_{dk} \mu_{dk})\right] > -2 \log \frac{\pi_1}{\pi_k} &- \sum_{d=1}^{D} \left[\log \frac{\beta_{d1}}{\beta_{dk}} - \beta_{d1} \mu_{d1}^2 + \beta_{dk} \mu_{dk}^2\right] \\
\end{align*}
\end{center}

Which considering the constant terms as $c$, we have:

\begin{align*}
    \sum_{d=1}^{D} \left[ x_d^2 (\beta_{dk} - \beta_{d1}) + 2 x_d (\beta_{d1} \mu_{d1} - \beta_{dk} \mu_{dk})\right] > c
\end{align*}

\newpage
\subsection*{j)}

Given this definition, I do not expect the region to be convex, since the definition assumes a mantaince of the linearity, 
but due to the quadratic term, this linearity falls apart. This can be shown by using the Hessian of the decision boundary,
but using the definition given in the question we can demonstrate it by the following: 

First, as pointed out, if a region $C_1$ is convex for any two points $\bm{x}_1, \bm{x}_2 \in C_1$ and any $\lambda \in [0, 1]$, then all the points that
lie in the line segment between $\bm{x}_1$ and $\bm{x}_2$ are also in $C_1$, meaning that the point: 

$$
\bm{x}_{\lambda} = \lambda \bm{x}_1 + (1 - \lambda) \bm{x}_2 \in C_1
$$

Now, let $\bm{x}_1 = (x_{11}, \cdots, x_{1D})$ and $\bm{x}_2 = (x_{21}, \cdots ,x_{2D})$ be two points in the region $C_1$, meaning that:

$$
\sum_{d=1}^{D} \left[ x_{1d}^2 (\beta_{dk} - \beta_{d1}) + 2 x_{1d} (\beta_{d1} \mu_{d1} - \beta_{dk} \mu_{dk})\right] > c
$$

and 

$$
\sum_{d=1}^{D} \left[ x_{2d}^2 (\beta_{dk} - \beta_{d1}) + 2 x_{2d} (\beta_{d1} \mu_{d1} - \beta_{dk} \mu_{dk})\right] > c
$$

Now, we need to check if $\bm{x}_{\lambda} = \lambda \bm{x}_1 + (1 - \lambda) \bm{x}_2$ is in $C_1$, i.e., if:

$$
\sum_{d=1}^{D} \left[ x_{\lambda d}^2 (\beta_{dk} - \beta_{d1}) + 2 x_{\lambda d} (\beta_{d1} \mu_{d1} - \beta_{dk} \mu_{dk})\right] > c
$$

Expanding the quadratic term, we have:

\begin{align*}
    x_{\lambda d}^2 &= \left(\lambda x_{1d} + (1 - \lambda) x_{2d}\right)^2 \\
    &= \lambda^2 x_{1d}^2 + 2 \lambda (1 - \lambda) x_{1d} x_{2d} + (1 - \lambda)^2 x_{2d}^2
\end{align*}

Expanding the linear term, we have:

\begin{align*}
    x_{\lambda d} &= \lambda x_{1d} + (1 - \lambda) x_{2d}
\end{align*}

Now, substituting the expanded terms in the inequality, we have:

\begin{align*}
    \sum_{d=1}^{D} \left[ \left(\lambda^2 x_{1d}^2 + 2 \lambda (1 - \lambda) x_{1d} x_{2d} + (1 - \lambda)^2 x_{2d}^2\right) (\beta_{dk} - \beta_{d1}) + 2 \left(\lambda x_{1d} + (1 - \lambda) x_{2d}\right) (\beta_{d1} \mu_{d1} - \beta_{dk} \mu_{dk})\right] &> c
\end{align*}

Now, we see that we could decompose in a linear combination of the previous inequalities for $\bm{x}_1$ and $\bm{x}_2$, which would respect the inequality, however, the quadratic is a cross term between $x_{1d}$ and $x_{2d}$,
making it non-linear separable, hence, the region is not convex, i.e., $\exists \lambda \in [0, 1]$ such that $x_{\lambda} \notin C_1$.

Taking the Hessian for the decision is a more straightforward way to evaluate the convexity, but it will be used in the next question.


\newpage
\subsection*{k)}
Considering the inequality showed before, we should expect that since it is a quadratic inequality, the decision boundary would create a region that is not convex.
If we eliminate the quadratic term, we would have a linear inequality, which would create a linear decision boundary, which is true if we consider the case where $\beta_{dk} = \beta_{dl}$ for $\forall k \neq l$.
However, I'm going to show this in the next paragraphs.

The decision boundary is the set of points where $p(C_1 | \bm{x}) = p(C_k | \bm{x})$, $\forall k \neq 1$.

Now, considering the inequality showed before we can say that the boundary for the region that a point $x$ belongs to class $C_1$ or $C_k$ is the set of points where the inequality becomes:

\begin{align*}
    \sum_{d=1}^{D} \left[ x_d^2 (\beta_{dk} - \beta_{d1}) + 2 x_d (\beta_{d1} \mu_{d1} - \beta_{dk} \mu_{dk})\right] = c
\end{align*}

Now, to evaluate the shape of the decision boundary, we can take the Hessian of the condition:

\begin{align*}
    H &= \begin{bmatrix}
        \frac{\partial^2}{\partial x_1^2} & \frac{\partial^2}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2}{\partial x_1 \partial x_D} \\
        \frac{\partial^2}{\partial x_2 \partial x_1} & \frac{\partial^2}{\partial x_2^2} & \cdots & \frac{\partial^2}{\partial x_2 \partial x_D} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial^2}{\partial x_D \partial x_1} & \frac{\partial^2}{\partial x_D \partial x_2} & \cdots & \frac{\partial^2}{\partial x_D^2} \\
    \end{bmatrix} \\
    H &= \begin{bmatrix}
        2 (\beta_{1k} - \beta_{11}) & 0 & \cdots & 0 \\
        0 & 2 (\beta_{2k} - \beta_{21}) & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 2 (\beta_{Dk} - \beta_{D1}) \\
    \end{bmatrix}
\end{align*}

Now, if the Hessian is positive definite, then the decision boundary is convex, otherwise, it is not. 
Hence, we need to evaluate the eigenvalues of the Hessian matrix, which, since it is a diagonal matrix, are the elements of the diagonal.

\begin{itemize}
    \setlength{\itemsep}{0pt} % Adjusts the space between items
    \setlength{\parskip}{0pt} % Adjusts the space between paragraphs
    \setlength{\leftskip}{15pt} % Adds forward shift (indentation)
    \item If the eigenvalues are all non negative, then the decision region inside the boundary is a convex region. 
    \item If the eigenvalues are all non positive, then the decision region inside the boundary is not a convex region.
    \item If the eigenvalues are all zero, then the decision region is a linear region.
\end{itemize}

This lead to the conclusion, only for part of the decision regions that separate the classes, that: 
\begin{itemize}
    \setlength{\itemsep}{0pt} % Adjusts the space between items
    \setlength{\parskip}{0pt} % Adjusts the space between paragraphs
    \setlength{\leftskip}{15pt} % Adds forward shift (indentation)
    \item if $\beta_{dk} > \beta_{d1} \forall d$, then the decision boundary is convex, meaning one of the regions is convex and, hence, the other is non-convex.
    \item if $\beta_{dk} < \beta_{d1} \forall d$, then the decision boundary is non-convex, meaning one of the regions is non-convex and, hence, the other is convex.
    \item if $\beta_{dk} = \beta_{d1} \forall d$, then the decision boundary is linear, and both regions are convex. 
\end{itemize}

\textbf{Note}: The boundary is a function, hence it can be convex or concave, however, both options lead to a convex and a non-convex region, because if the boundary is convex, then the region inside the boundary is convex and the region outside the boundary is non-convex, and vice-versa. If it's linear, then both regions are convex.
Also, we need to bare in mind that we are only evaluating the regions that separate the classes, and not the overall problem, if the region is convex relative to class $C_2$, it does not mean that it is convex relative to class $C_3$.

Well, now we need to consider the overall problem and see all the decision regions. For that we need to have in mind that
if all the subboundaries of a class $C_k$ are convex, then the decision region for that class is convex, and if one of the subboundaries is non-convex, then the decision region is non-convex. 
With this, we can say that, unless all the decision boundaries are linear, we can have at most one convex region and $K - 1$ non-convex regions. because if one region is convex relative to all the other classes, it means that all the other classes would have a boundary where the region is non-convex.
Meaning, that in general, and without the assumption of the hint, the problem becomes non-convex.

Now, considering the hint, we have that $\beta_{d1} = \beta_{dk}$, $\forall d, k$, then the decision boundaries are linear, 
and, hence we would end with a set of linar decision regions which would all be convex. 

This also makes sense relating to the previous question that was meant to show the quadratic inequality that would define the decision boundary.
If we consider the case where $\beta_{dk} != \beta_{dl}$ for $k \neq l$, then the inequality would be a quadratic inequality. 
Whereas, if we consider the case where $\beta_{dk} = \beta_{dl}$ for $k \neq l$, then the inequality would be a linear inequality.


\newpage
\section{Binary Classification}

\subsection*{a)}

\subsubsection*{i)}

The dataset above \textbf{cannot} be classified using a logistic regression model with linear features, since the data is \textbf{not linearly separable}, 
meaning that there is no line that can separate the two classes.

Assuming we have points $A (1, 1)$, $B (-1, -1)$, $C (1, -1)$ and $D (-1, 1)$, we can see that the points $A$ and $B$ belong to class \textit{Blue} and the points $C$ and $D$ belong to class \textit{Orange}.
If we now assume a line that separates the two classes, this boundary could be defined as $ax + by + c = 0$, where $a$, $b$ and $c$ are the parameters of the line.
We would know that for the points to be on the same side of the line, then: 

\begin{align*}
    (ax_1 + by_1 + c) \cdot (ax_2 + by_2 + c) > 0
\end{align*}

This because if a point belongs to a class, let's say the class above the line, putting in the equation of the line would give a positive value, and the same for the other point.
For the other class, the value would be negative. Hence, we know that the product of the two values should be positive, if the points belong to the same class.
Saying this, considering points $A$ and $B$, we would have:

\begin{align*}
    (a + b + c) \cdot (-a - b + c) > 0 \\
\end{align*}

Now, considering points $A$ and $D$, we would have:

\begin{align*}
    (a + b + c) \cdot ( a - b - c) > 0 \\
\end{align*}

Now, considering points $D$ and $C$, we would have:

\begin{align*}
    (- a + b + c) \cdot (a - b + c) > 0 \\
\end{align*}

We can combain them to have:

\begin{align*}
    (a + b + c)^2 \cdot (-a - b + c) \cdot ( a - b - c) > 0 \\ 
\end{align*}

Meaning, that $(- a - b + c)$ and $(a - b - c)$ should have the same sign, as $(- a + b + c)$ and $(a - b + c)$ should have the same sign,
which gives us a contradiction. Meaning that the data is not linearly separable.

\newpage
\subsubsection*{ii)}

The data \textbf{can} be classified using a logistic regression model with \textbf{non-linear basis functions},
depending on the basis functions that are used.

For example, consider the function $\Phi \in \mathbb{R}^2 \rightarrow \mathbb{R}^3$ defined as $\Phi(x_1, x_2) = (x_1, x_2, x_1 x_2)$.
Now, we know that to belong to class \textit{Orange}, the value of $x_1 x_2$ should be negative, and to belong to class \textit{Blue}, the value of $x_1 x_2$ should be positive, 
meaning that in this higher space the data is linearly separable by the plane $x_1 x_2 = 0$, which is the third dimension of the space. 
We could also use $\Phi \in \mathbb{R}^2 \rightarrow \mathbb{R}^1$ defined as $\Phi(x_1, x_2) = (x_1 x_2)$, which would also make the data linearly separable, or even other basis functions. 
Basically, the idea is to transform the data into a different space where it is linearly separable, and then use the logistic regression model to classify the data.

\subsubsection*{iii)}

The data \textbf{can} be classified using a \textbf{Multilayer Perceptron with one hidden layer}. 
While a single Perceptron is not able to classify the data, since the data is not linearly separable as shown before,
a Multilayer Perceptron with one hidden layer can classify the data, since it can learn non-linear decision boundaries.
For example, let's look at the architecture of a MLP with one hidden layer with 2 neurons. Neuron 1 could activate
for the inputs that belong to class \textit{Orange} and neuron 2 could activate for the inputs that belong to class \textit{Blue}. i.e., 
two neurons would create two linear decision boundaries, which would allow the MLP to classify the data. 

\newpage
\subsection*{b)}

The data \textit{cannot} be classified using a \textit{Naive Bayes Classifier} without changing the Basis. This happens because the Naive Bayes
Classifier assumes that the features are independent given the class. However, in this case, if we know the class
and know the value of $x_1$, we can infer the sign of $x_2$, and vice-versa, i.e., the features are not independent.
For example, if we know that $x$ belongs to class \textit{Orange}, and we know the sign of $x_1$, we can infer the sign of $x_2$, which breaks the assumption of NB. 

Given the subset of points $A (1, 1)$, $B (-1, -1)$, $C (1, -1)$ and $D (-1, 1)$, we can see that the points $A$ and $B$ belong to class \textit{Blue} and the points $C$ and $D$ belong to class \textit{Orange}.
Now, assuming only this subset of points and knowing that Naive Bayes assumes that: 

\begin{align*}
    p(x_1, x_2 | C_k) &= p(x_1 | C_k) p(x_2 | C_k)
\end{align*}

Then, for class orange and assuming a frequentist approach on the probabilities, we have that: 

\begin{align*}
    p(x_1 = 1 | C_{\text{Orange}}) &= \frac{1}{2} \quad \text{, since we have 1 sample with $x_1 = -1$ and 1 sample with $x_1 = 1$} \\
    p(x_2 = 1 | C_{\text{Orange}}) &= \frac{1}{2} \quad \text{, since we have 1 sample with $x_2 = -1$ and 1 sample with $x_2 = 1$} \\
    p(x_1 = 1, x_2 = 1 | C_{\text{Orange}}) &= p(x_1 = 1 | C_{\text{Orange}}) p(x_2 = 1 | C_{\text{Orange}}) = \frac{1}{4} \quad \text{, assuming the Naives Bayes assumption}
\end{align*}

However, the probability of $p(x_1 = 1, x_2 = 1 | C_{\text{Orange}})$ is 0, since we do not have any sample that belongs to class \textit{Orange}. 

This shows that the assumption of the Naive Bayes Classifier falls, hence we cannot use it to classify the data.

However, if we do previously a change of basis, for example to the basis $\Phi(x_1, x_2) = (r, \tan \theta)$, where $r = \sqrt{x_1^2 + x_2^2}$ and $\theta = \arctan\left(\frac{x_2}{x_1}\right)$,
then the data becomes linearly separable and the features become independent, e.g., if you know the radius of the point, you can infer nothing about the angle of the point and vice-versa.
Considering the subset of points $A (1, 1)$, $B (-1, -1)$, $C (1, -1)$ and $D (-1, 1)$. Transforming to the new basis, we have that:

$A(1, 1) \rightarrow A(\sqrt{2}, 1)$, $B(-1, -1) \rightarrow B(\sqrt{2}, 1)$, $C(1, -1) \rightarrow C(\sqrt{2}, -1)$ and $D(-1, -1) \rightarrow D(\sqrt{2}, -1)$.

Now, 
$p(A | C_{\text{Orange}}) = p(x_1 = \sqrt{2} | C_{\text{Orange}}) p(x_2 = 1 | C_{\text{Orange}}) = 1 \cdot 1 = p(x_1 = \sqrt{2}, x_2 = 1 | C_{\text{Orange}})$
And we will get the same for all the other points.

\newpage
\subsection*{c)}

The dataset consists of a inner circle of points that belong to class \textit{Orange} and
an outer annulus of points that belong to class \textit{Purple}. If we perform a change of basis to polar coordinates,
keeping the dimension of the problem, we can see that the data is linearly separable, since the radius of the points
that belong to class \textit{Orange} is smaller than the radius of the points that belong to class \textit{Purple} and the
angle of the points does not matter. Hence, we can say that knowing the angle of the point gives no information
about the radius and vice-versa, which makes the data linearly separable and independent in polar coordinates. This is the same reasoning as before,
if we change the basis to a basis where the features are independent given the class, then the data becomes linearly separable in an independent way and the Naive Bayes Classifier can be applied.

Putting in on a mathematical form:

\begin{align*}
    r = \sqrt{x^2 + y^2} \\
    \theta = \arctan\left(\frac{y}{x}\right)
\end{align*}

For class \textit{Orange}, we have that:

\begin{align*}
    r \in [0, r_1] \\
    \theta \in [0, 2\pi]
\end{align*}

For class \textit{Purple}, we have that:

\begin{align*}
    r \in [r_1, r_2] \\
    \theta \in [0, 2\pi]
\end{align*}

Assuming we have uniform distributions for the radius and the angle, we now have that: 

\begin{align*}
    p(r | C_{\text{Orange}}) &= \begin{cases}
        \frac{1}{r_1} & \text{if } r \in [0, r_1] \\
        0 & \text{otherwise}
    \end{cases} \\
    p(\theta | C_{\text{Orange}}) &= \frac{1}{2\pi} \\
    p(r | C_{\text{Purple}}) &= \begin{cases}
        \frac{1}{r_2 - r_1} & \text{if } r \in [r_1, r_2] \\
        0 & \text{otherwise}
    \end{cases} \\
    p(\theta | C_{\text{Purple}}) &= \frac{1}{2\pi}
\end{align*}

Now, in Naive Bayes we assume that the features are independent given the class, hence we have that:

\begin{align*}
    p(r, \theta | C_{\text{Orange}}) &= p(r | C_{\text{Orange}}) p(\theta | C_{\text{Orange}}) \\
    p(r, \theta | C_{\text{Purple}}) &= p(r | C_{\text{Purple}}) p(\theta | C_{\text{Purple}})
\end{align*}

Now, applying the Bayes rule, we have that:

\begin{align*}
    p(C_{\text{Orange}} | r, \theta) &= \frac{p(r, \theta | C_{\text{Orange}}) p(C_{\text{Orange}})}{p(r, \theta)} \\
    p(C_{\text{Purple}} | r, \theta) &= \frac{p(r, \theta | C_{\text{Purple}}) p(C_{\text{Purple}})}{p(r, \theta)}
\end{align*}

Assuming Naive Bayes, we have that: 

\begin{align*}
    p(C_{\text{Orange}} | r, \theta) &= \frac{p(r | C_{\text{Orange}}) p(\theta | C_{\text{Orange}}) p(C_{\text{Orange}})}{p(r, \theta)} \\
    p(C_{\text{Purple}} | r, \theta) &= \frac{p(r | C_{\text{Purple}}) p(\theta | C_{\text{Purple}}) p(C_{\text{Purple}})}{p(r, \theta)}
\end{align*}

Now, substituting with the distributions, we have that:

\begin{align*}
    p(C_{\text{Orange}} | r, \theta) &= \begin{cases}
        \frac{\frac{1}{r_1} \frac{1}{2\pi} p(C_1)}{p(r, \theta)} & \text{if } r \in [0, r_1] \\
        0 & \text{otherwise} 
    \end{cases} \\
    p(C_{\text{Purple}} | r, \theta) &= \begin{cases}
        \frac{\frac{1}{r_2 - r_1} \frac{1}{2\pi} p(C_2)}{p(r, \theta)} & \text{if } r \in [r_1, r_2] \\
        0 & \text{otherwise}
    \end{cases}
\end{align*}

Which simplifies to the fact that we only need to know the radius of the point to classify it
and the angle does not give any information about the class of the point.

\newpage
\subsection*{d)}

In \textbf{Logistic Regression with Nonlinear Basis Functions}, $\phi(x)$ is a fixed and predetermined function
that is applied to the input features, i.e., this transformation does not change during the training process
and the weights are learned in the transformed space, meaning it only learns the weights associated with the
transformed features.

In \textbf{Multilayer Perceptron with One Hidden Layer}, the $\phi$ transformation is learned during the training process,
along its hidden layers. The MLP adjusts the weights and biases within each layer allowing it to learn complex transformations that are
the cumulative effect of the transformations learned in each layer. This allows the MLP to learn more complex decision boundaries than the Logistic Regression,
since it is not bound to the fixed transformation of the features, that, in some cases, might even be unknown or hard to determine.

\newpage
\section{Regularized Logistic Regression}

\subsection*{a)}

Considering a logistic regression for $K$ classes with $N$ training vectores $\bm{x}_n$ mapped by a function $\phi(x_n)$ to $M$-dimensional space, given that $p(C_k | \phi(x), \bm{w_1}, \cdots, \bm{w_K}) = y_k(\phi) = \frac{\exp(w_k^T \phi)}{\sum_{j=1}^{K} \exp(w_j^T \phi)}$ and given an assumption of a Gaussian prior on the vectors $\bm{w_1}, \cdots, \bm{w_K}$:

\begin{align*}
    p(\bm{w_1}, \cdots, \bm{w_K} | \alpha) &= \mathcal{N}(w_k | 0, \alpha^{-1} I)
\end{align*}

We can then write the likelihood $p(T | \Phi, \bm{w_1}, \cdots, \bm{w_K})$ as:

\begin{align*}
    p(T | \Phi, \bm{w_1}, \cdots, \bm{w_K}) &= \prod_{n=1}^{N} p(t_n | \phi_n, \bm{w_1}, \cdots, \bm{w_K}) \quad {(\text{since the data is i.i.d.})} \\
    &= \prod_{n=1}^{N} \prod_{k=1}^{K} p(t_{nk} = 1 | \phi_n, \bm{w_1}, \cdots, \bm{w_K})^{t_{nk}} \\
    &= \prod_{n=1}^{N} \prod_{k=1}^{K} p(C_k | \phi_n, \bm{w_1}, \cdots, \bm{w_K})^{t_{nk}} \quad {(\text{since } t_{nk} = 1 \text{ if } x_n \text{ belongs to class } k)} \\
    &= \prod_{n=1}^{N} \prod_{k=1}^{K} y_k(\phi_n)^{t_{nk}} \\
    &= \prod_{n=1}^{N} \prod_{k=1}^{K} \left(\frac{\exp(\bm{w_k}^T \phi_n)}{\sum_{j=1}^{K} \exp(\bm{w_j}^T \phi_n)}\right)^{t_{nk}}
\end{align*}

Writing down the log likelihood, we have:

\begin{align*}
    \log p(T | \Phi, \bm{w_1}, \cdots, \bm{w_K}) &= \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \log y_k(\phi_n) \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \log \left(\frac{\exp(\bm{w_k}^T \phi_n)}{\sum_{j=1}^{K} \exp(\bm{w_j}^T \phi_n)}\right) \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \left(\bm{w_k}^T \phi_n - \log \sum_{j=1}^{K} \exp(\bm{w_j}^T \phi_n)\right)
\end{align*}

\newpage
\subsection*{b)}

The prior in the question is given as: 

\begin{align*}
    p(\bm{w_1}, \cdots, \bm{w_K} | \alpha) &= \prod_{k=1}^{K} \mathcal{N}(\bm{w_k} | 0, \alpha^{-1} \bm{I}) \\
    &= \prod_{k=1}^{K} \left(\frac{\alpha}{2\pi}\right)^{\frac{M}{2}} \exp\left(-\frac{\alpha}{2} \bm{w_k}^T \bm{w_k}\right) \\
    &= \left(\frac{\alpha}{2\pi}\right)^{\frac{M}{2} K} \exp\left(-\frac{\alpha}{2} \sum_{k=1}^{K} \bm{w_k}^T \bm{w_k}\right)
\end{align*}

Writing down the log prior, we have:

\begin{align*}
    \log p(\bm{w_1}, \cdots, \bm{w_K} | \alpha) &= \log \left( \left(\frac{\alpha}{2\pi}\right)^{\frac{M}{2} K} \exp\left(-\frac{\alpha}{2} \sum_{k=1}^{K} \bm{w_k}^T \bm{w_k}\right)\right) \\
    &= \frac{M}{2} K \log \frac{\alpha}{2\pi} - \frac{\alpha}{2} \sum_{k=1}^{K} \bm{w_k}^T \bm{w_k} \\
    &= \frac{M}{2} K \log \frac{\alpha}{2\pi} - \frac{\alpha}{2} \sum_{k=1}^{K} \sum_{m=0}^{M - 1} w_{km}^2
\end{align*}

Computing the logarithm helps us in two ways when we are doing the computations. Firstly, it avoids numerical
underflows specially when dealing with small numbers that can arise due to small probabilities being multiplied, and secondly,
it helps in the computations since the logarithm of a product is the sum of the logarithms of the terms, which makes
the computations easier and, sometimes, reduce the problem to a multiplication of matrices.

\newpage
\subsection*{c)}

The posterior distribution is given by:

\begin{align*}
    p(\bm{w_1}, \cdots, \bm{w_K} | T, \Phi, \alpha) &\propto p(T | \Phi, \bm{w_1}, \cdots, \bm{w_K}) p(\bm{w_1}, \cdots, \bm{w_K} | \alpha) \quad {(\text{by Bayes' theorem})} \\
    &= \prod_{n=1}^{N} \prod_{k=1}^{K} y_k(\phi_n)^{t_{nk}} \prod_{k=1}^{K} \left(\frac{\alpha}{2\pi}\right)^{\frac{M}{2}} \exp\left(-\frac{\alpha}{2} \bm{w_k}^T \bm{w_k}\right) \\
    &= \prod_{n=1}^{N} \prod_{k=1}^{K} \left(\frac{\exp(\bm{w_k}^T \phi_n)}{\sum_{j=1}^{K} \exp(\bm{w_j}^T \phi_n)}\right)^{t_{nk}} \prod_{k=1}^{K} \left(\frac{\alpha}{2\pi}\right)^{\frac{M}{2}} \exp\left(-\frac{\alpha}{2} \bm{w_k}^T \bm{w_k}\right) \\
    &= \prod_{n=1}^{N} \prod_{k=1}^{K} \left(\frac{\exp(\bm{w_k}^T \phi_n)}{\sum_{j=1}^{K} \exp(\bm{w_j}^T \phi_n)}\right)^{t_{nk}} \left(\frac{\alpha}{2\pi}\right)^{\frac{M}{2} K} \exp\left(-\frac{\alpha}{2} \sum_{k=1}^{K} \bm{w_k}^T \bm{w_k}\right) \\
    &= \prod_{n=1}^{N} \prod_{k=1}^{K} \left(\frac{\exp(\bm{w_k}^T \phi_n)}{\sum_{j=1}^{K} \exp(\bm{w_j}^T \phi_n)}\right)^{t_{nk}} \left(\frac{\alpha}{2\pi}\right)^{\frac{M}{2} K} \exp\left(-\frac{\alpha}{2} \sum_{k=1}^{K} \sum_{m=0}^{M - 1} w _{km}^2\right) \\
\end{align*}

\textbf{Note}: Here, we used the fact that the posterior distribution is proportional to the product of the likelihood and the prior, since the denominator of the Bayes' theorem is a constant that does not depend on the weights.
Besides that, it is hard to compute due to their integral nature and in the end it would be just a normalization constant that would not affect the weights. The result would be that:

$$
p(\bm{w_1}, \cdots, \bm{w_K} | T, \Phi, \alpha) = \frac{p(T | \Phi, \bm{w_1}, \cdots, \bm{w_K}) p(\bm{w_1}, \cdots, \bm{w_K} | \alpha)}{p(T | \Phi, \alpha)}
$$
With
$$
p(T | \Phi, \alpha) = \int p(T | \Phi, \bm{w_1}, \cdots, \bm{w_K}) p(\bm{w_1}, \cdots, \bm{w_K} | \alpha) \bm{dw_1}, \cdots, \bm{dw_K}
$$

\newpage
\subsection*{d)}

To find the maximum of the posterior distribution, we would need to find:

\begin{align*}
    &argmax_{\bm{w_1}, \cdots, \bm{w_K}} p(\bm{w_1}, \cdots, \bm{w_K} | T, \Phi, \alpha) \\
\end{align*}

Which is equal to find the maximum of the log posterior distribution:

\begin{align*}
    &argmax_{\bm{w_1}, \cdots, \bm{w_K}} \log p(\bm{w_1}, \cdots, \bm{w_K} | T, \Phi, \alpha) \\
    &argmax_{\bm{w_1}, \cdots, \bm{w_K}} \log p(T | \Phi, \bm{w_1}, \cdots, \bm{w_K}) + \log p(\bm{w_1}, \cdots, \bm{w_K} | \alpha) \\
\end{align*}

We can ignore the log of the evidence term, since it does not depend on the weights.

This then give us: 

\begin{align*}
    &argmax_{\bm{w_1}, \cdots, \bm{w_K}} \log p(\bm{w_1}, \cdots, \bm{w_K} | T, \Phi, \alpha) \\
    &= argmax_{\bm{w_1}, \cdots, \bm{w_K}} \left(\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \log y_k(\phi_n) + \frac{M}{2} K \log \frac{\alpha}{2\pi} - \frac{\alpha}{2} \sum_{k=1}^{K} \sum_{m=0}^{M - 1} w_{km}^2\right)
\end{align*}


Now, we can drop the constant terms since they do not depend on the weights: 

\begin{align*}
    &argmax_{\bm{w_1}, \cdots, \bm{w_K}} \log p(\bm{w_1}, \cdots, \bm{w_K} | T, \Phi, \alpha) \\
    &= argmax_{\bm{w_1}, \cdots, \bm{w_K}} \left(\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \log y_k(\phi_n) - \frac{\alpha}{2} \sum_{k=1}^{K} \sum_{m=0}^{M - 1} w_{km}^2\right)
\end{align*}

Taking the argmax of the log posterior distribution is equivalent to taking the argmin of the negative log posterior distribution, hence we have:

\begin{align*}
    &argmin_{\bm{w_1}, \cdots, \bm{w_K}} -\log p(\bm{w_1}, \cdots, \bm{w_K} | T, \Phi, \alpha) \\
    &= argmin_{\bm{w_1}, \cdots, \bm{w_K}} \left(-\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \log y_k(\phi_n) + \frac{\alpha}{2} \sum_{k=1}^{K} \sum_{m=0}^{M - 1} w_{km}^2\right)
\end{align*}

Which reduces the problem of minimizing the function: 

\begin{align*}
    E(\bm{w_1}, \cdots, \bm{w_K}) &= -\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \log y_k(\phi_n) + \frac{\alpha}{2} \sum_{k=1}^{K} \sum_{m=0}^{M - 1} |w_{km}|^2
\end{align*}

with respect to $\bm{w_1}, \cdots, \bm{w_K}$ as we wanted to show.

\newpage
\subsection*{e)}

If I'm certain that my weights should lie around zero, then I would choose a high value for $\alpha$. This because
if I'm certain about my weights and if they are modelled as a Gaussian distribution, then I want the most part of the
distribution to be around the mean (zero), which means a smaller variance. Since the variance is inversely proportional to $\alpha$,
then I would choose a high value for $\alpha$. This would make the weights to be close to zero, because
since we are trying to minimize the function in the previous question, the term $\frac{\alpha}{2} \sum_{k=1}^{K} \sum_{m=0}^{M - 1} w_{km}^2$
would also be minimized. The term is always positive, hence if $\alpha$ is high, then the weights would need to be close to zero to minimize the function
and balance the increase in the term $\frac{\alpha}{2} \sum_{k=1}^{K} \sum_{m=0}^{M - 1} w_{km}^2$ done by the high value of $\alpha$.

\newpage
\subsection*{f)}

If I'm very uncertain about my weights, then the distribution for the weights should have a high variance, 
resembling a uniform distribution. This would mean that $\alpha$ should be small, since the variance is inversely proportional to $\alpha$.
The more uncertain I'm, the more $\alpha$ is small, so as $\alpha \rightarrow 0$, the equation at (d):

\begin{align*}
    \lim_{\alpha \rightarrow 0} -\log p(\bm{w_1}, \cdots, \bm{w_K} | T, \Phi, \alpha) &= \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \log \left(\frac{\exp(\bm{w_k}^T \phi_n)}{\sum_{j=1}^{K} \exp(\bm{w_j}^T \phi_n)}\right)
\end{align*}

Which is actually the MLE solution. 

\newpage
\subsection*{g)}

Taking the gradient of the log-likelihood with respect to $\bm{w_j}$:

\begin{align*}
    \nabla_{\bm{w_j}} \log p(T, \Phi, \bm{w_1}, \cdots, \bm{w_K}) &= \nabla_{\bm{w_j}} \left(\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \log y_k(\bm{\phi_n})\right) \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \nabla_{\bm{w_j}} \log y_k(\bm{w_k}^T \bm{\phi_n}) \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \frac{1}{y_k(\bm{w_k}^T \bm{\phi_n})} \nabla_{\bm{w_j}} y_k(\bm{w_k}^T \bm{\phi_n}) \quad \text{, since y is the softmax function} \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \frac{1}{y_k(\bm{w_k}^T \bm{\phi_n})} y_k(\bm{w_k}^T \bm{\phi_n}) (\mathds{1}_{kj} - y_k(\bm{w_k}^T \bm{\phi_n})) \nabla_{\bm{w_j}} \bm{w_k}^T \bm{\phi_n}^T \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} (\mathds{1}_{kj} - y_j(\bm{w_j}^T \bm{\phi_n})) \bm{\phi_n}^T \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \mathds{1}_{kj} \bm{\phi_n}^T - \sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} y_j(\bm{w_j}^T \bm{\phi_n}) \bm{\phi_n}^T \\
    &= \sum_{n=1}^{N} t_{nj} \bm{\phi_n}^T - \sum_{n=1}^{N} y_j(\bm{w_j}^T \bm{\phi_n}) \bm{\phi_n}^T \sum_{k=1}^{K} t_{nk} \\
    &= \sum_{n=1}^{N} t_{nj} \bm{\phi_n}^T - \sum_{n=1}^{N} y_j(\bm{w_j}^T \bm{\phi_n}) \bm{\phi_n}^T \quad \text{, since } \sum_{k=1}^{K} t_{nk} = 1 \\
    &= \sum_{n=1}^{N} (t_{nj} - y_j(\bm{w_j}^T \bm{\phi_n})) \bm{\phi_n}^T
\end{align*}

\newpage
\subsection*{h)}

Now taking the gradient of the log-prior:

\begin{align*}
    \nabla_{w_j} \log p(\bm{w_1}, \cdots, \bm{w_K} | \alpha) &= \nabla_{w_j} \left(\frac{M}{2} K \log \frac{\alpha}{2\pi} - \frac{\alpha}{2} \sum_{k=1}^{K} \sum_{m=0}^{M - 1} w_{km}^2\right) \\
    &= \nabla_{w_j} \left(- \frac{\alpha}{2} \sum_{m=0}^{M - 1} w_{jm}^2\right) \\
    &= \nabla_{w_j} \left(- \frac{\alpha}{2} \bm{w_j}^T \bm{w_j}\right) \\
    &= - \alpha \bm{w_j}^T
\end{align*}

This gives a row vector, as expected since we are taking the gradient of a scalar that comes from the distribution with respect to a vector.

Now, taking the gradient of the log-posterior is just the sum of the gradients of the log-likelihood and the log-prior,
since the gradient is a linear operator:

\begin{align*}
    \nabla_{w_j} \log p(\bm{w_1}, \cdots, \bm{w_K} | T, \Phi, \alpha) &= \nabla_{w_j} \log p(T, \Phi, \bm{w_1}, \cdots, \bm{w_K}) + \nabla_{w_j} \log p(\bm{w_1}, \cdots, \bm{w_K} | \alpha) \\
    &= \sum_{n=1}^{N} (t_{nj} - y_j(\bm{w_j}^T \bm{\phi_n})) \bm{\phi_n}^T - \alpha \bm{w_{j}}^T
\end{align*}







\clearpage

\appendix


\newpage
\printbibliography

\end{document}

